{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "Yeah ! Let's start with our actual project. In this assignment we will load the database and do preprocessing tasks.\n",
    "Ensure you have following packages installed\n",
    "1. numpy\n",
    "2. pandas  \n",
    "( Hope you are familiar with above two modules well )\n",
    "3. nltk (don't worry, we just need this to remove stopwords while preprocessing)\n",
    "4. tensorflow\n",
    "5. keras\n",
    "6. scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Importing essential libraries and functions\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import io\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the IMDB Reviews Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (50000, 2)\n",
      "Columns: Index(['review', 'sentiment'], dtype='object')\n",
      "Missing data:\n",
      "review       0\n",
      "sentiment    0\n",
      "dtype: int64\n",
      "After removing missing data:\n",
      "Dataset Shape: (50000, 2)\n"
     ]
    }
   ],
   "source": [
    "# TODO:\n",
    "'''\n",
    "Using pandas load the imdb reviews csv file\n",
    "Analyse its shape and columns\n",
    "Check for missing data and remove those rows\n",
    "'''\n",
    "\n",
    "# Loading the IMDB Reviews Dataset\n",
    "dataset_path = 'IMDB_dataset.csv'\n",
    "\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns)\n",
    "\n",
    "# checking for missing data\n",
    "print(\"Missing data:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# removing such rows\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#checking \n",
    "print(\"After removing missing data:\")\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : Complete the function to preprocess the text data\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    # First make the sentence lowercase\n",
    "    sentence = sentence.lower()\n",
    "    # Remove all html tags from the sentence i.e replace anything between <> with space\n",
    "    # Hint use Regular Expression i.e. re.sub()\n",
    "    sentence = re.sub(r'<.*?>', ' ', sentence)\n",
    "    \n",
    "    # Remove all special characters i.e. anything other than alphabets and numbers. Replace them with space\n",
    "    sentence = re.sub(r'[^a-zA-Z0-9\\s]', ' ', sentence)\n",
    "    \n",
    "    # Remove all single characters i.e. a-z and A-Z and Replace them with space\n",
    "    sentence = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', sentence)\n",
    "    \n",
    "    # Remove all multiple spaces and replace them with single space\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    \n",
    "    # Use the nltk library to remove all stopwords from the sentence\n",
    "    # stopwords are the words like and, the, is, are etc.\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    sentence = ' '.join(words)\n",
    "    \n",
    "    # return the sentence\n",
    "    return sentence\n",
    "    \n",
    "    pass # remove this line after writing the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one reviewers mentioned watching 1 oz episode hooked right exactly happened first thing struck oz brutality unflinching scenes violence set right word go trust show faint hearted timid show pulls punches regards drugs sex violence hardcore classic use word called oz nickname given oswald maximum security state penitentary focuses mainly emerald city experimental section prison cells glass fronts face inwards privacy high agenda em city home many aryans muslims gangstas latinos christians italians irish scuffles death stares dodgy dealings shady agreements never far away would say main appeal show due fact goes shows dare forget pretty pictures painted mainstream audiences forget charm forget romance oz mess around first episode ever saw struck nasty surreal say ready watched developed taste oz got accustomed high levels graphic violence violence injustice crooked guards sold nickel inmates kill order get away well mannered middle class inmates turned prison bitches due lack street skills prison experience watching oz may become comfortable uncomfortable viewing thats get touch darker side', 'wonderful little production filming technique unassuming old time bbc fashion gives comforting sometimes discomforting sense realism entire piece actors extremely well chosen michael sheen got polari voices pat truly see seamless editing guided references williams diary entries well worth watching terrificly written performed piece masterful production one great master comedy life realism really comes home little things fantasy guard rather use traditional dream techniques remains solid disappears plays knowledge senses particularly scenes concerning orton halliwell sets particularly flat halliwell murals decorating every surface terribly well done', 'thought wonderful way spend time hot summer weekend sitting air conditioned theater watching light hearted comedy plot simplistic dialogue witty characters likable even well bread suspected serial killer may disappointed realize match point 2 risk addiction thought proof woody allen still fully control style many us grown love laughed one woody comedies years dare say decade never impressed scarlet johanson managed tone sexy image jumped right average spirited young woman may crown jewel career wittier devil wears prada interesting superman great comedy go see friends', 'basically family little boy jake thinks zombie closet parents fighting time movie slower soap opera suddenly jake decides become rambo kill zombie ok first going make film must decide thriller drama drama movie watchable parents divorcing arguing like real life jake closet totally ruins film expected see boogeyman similar movie instead watched drama meaningless thriller spots 3 10 well playing parents descent dialogs shots jake ignore', 'petter mattei love time money visually stunning film watch mr mattei offers us vivid portrait human relations movie seems telling us money power success people different situations encounter variation arthur schnitzler play theme director transfers action present time new york different characters meet connect one connected one way another next person one seems know previous point contact stylishly film sophisticated luxurious look taken see people live world live habitat thing one gets souls picture different stages loneliness one inhabits big city exactly best place human relations find sincere fulfillment one discerns case people encounter acting good mr mattei direction steve buscemi rosario dawson carol kane michael imperioli adrian grenier rest talented cast make characters come alive wish mr mattei good luck await anxiously next work']\n"
     ]
    }
   ],
   "source": [
    "# TODO :\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "# Call the preprocessing function for each review in the dataframe and\n",
    "# save the results in a new list of preprocessed_reviews\n",
    "preprocessed_reviews = [preprocessing(review) for review in df['review']]\n",
    "# This list will be your input to the neural network\n",
    "# We will call this list as X from now on\n",
    "X = preprocessed_reviews\n",
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# TODO :\n",
    "# Convert sentiment column in the dataframe to numbers\n",
    "# Convert positive to 1 and negative to 0 and store it in numpy array\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "# We will call this numpy array as y from now on\n",
    "y = np.array(df['sentiment'])\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: 40000 40000\n",
      "Testing data shape: 10000 10000\n"
     ]
    }
   ],
   "source": [
    "# TODO : Split the data into training and testing (80-20 ratio)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# The train set will be used to train our deep learning models \n",
    "# while test set will be used to evaluate how well our model performs \n",
    "print(\"Training data shape:\", len(X_train), len(y_train))\n",
    "print(\"Testing data shape:\", len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing embedding layer\n",
    "Let's now write the script for our embedding layer. Embedding layer converts our textual data into numeric form. It is then **used as the first layer for the deep learning model like LSTM**.  \n",
    "To know more about word embedding you may refer to following video\n",
    "https://www.youtube.com/watch?v=9S0-OC4LFNo  \n",
    "#### Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92321"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "word_tokenizer = Tokenizer()\n",
    "# TODO: Fit the tokenizer on the training data (X_train)\n",
    "word_tokenizer.fit_on_texts(X_train)\n",
    "# TODO: Convert training data to sequences of integers\n",
    "# Hint: Use texts_to_sequences method\n",
    "X_train_sequences = word_tokenizer.texts_to_sequences(X_train)\n",
    "# TODO: Convert test data to sequences of integers\n",
    "# Hint: Use texts_to_sequences method\n",
    "X_test_sequences = word_tokenizer.texts_to_sequences(X_test)\n",
    "# End TODO\n",
    "# Saving the tokenizer in a json file (Already done for you)\n",
    "# This will be used later for prediction on data in next assignments\n",
    "tokenizer_json = word_tokenizer.to_json()\n",
    "with io.open('b3_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "    \n",
    "# Vocab_length is the number of unique words in our dataset\n",
    "# Adding 1 to store dimensions for words for which no pretrained word embeddings exist\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "vocab_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded training data shape: (40000, 100)\n",
      "Padded test data shape: (10000, 100)\n"
     ]
    }
   ],
   "source": [
    "# Padding all reviews to be of same length 'maxlen' words\n",
    "maxlen = 100\n",
    "# You can try different dimensions like 50, 100, 200 and 300\n",
    "# and see how the model performs in next week\n",
    "\n",
    "# TODO: Pad the training data sequences\n",
    "# Hint: Use pad_sequences with 'post' padding and maxlen=maxlen\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "X_train_padded = pad_sequences(X_train_sequences, padding='post', maxlen=maxlen)\n",
    "\n",
    "# TODO: Pad the test data sequences\n",
    "# Hint: Use pad_sequences with 'post' padding and maxlen=maxlen\n",
    "X_test_padded = pad_sequences(X_test_sequences, padding='post', maxlen=maxlen)\n",
    "print(\"Padded training data shape:\", X_train_padded.shape)\n",
    "print(\"Padded test data shape:\", X_test_padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (92321, 100)\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty dictionary for embeddings\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "# Open the GloVe file (a2_glove.6B.100d.txt) with utf-8 encoding\n",
    "glove_file = open('a2_glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "glove_file.close()\n",
    "\n",
    "# TODO : Create an embedding matrix where each row corresponds to the index of the\n",
    "# unique word in the dataset and each column corresponds to the word vector\n",
    "# in the GloVe embedding \n",
    "# So the matrix will have vocab_length rows and maxlen columns\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((vocab_length, embedding_dim))\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    if index < vocab_length:\n",
    "        embedding_vector = embeddings_dictionary.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "print(\"Embedding matrix shape:\", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
